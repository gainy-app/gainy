version: '3.6'
services:
  meltano-airflow-scheduler:
    restart: always
    env_file:
      - .env
    build:
      context: ./src/gainy-fetch
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    volumes:
      - ./src/gainy-fetch/meltano/scripts:/project/scripts:delegated
      - ./src/gainy-fetch/meltano/meltano.template.yml:/project/meltano.template.yml:delegated
      - ./src/gainy-fetch/meltano/transform:/project/transform:delegated
      - ./src/gainy-fetch/meltano/data:/project/data:delegated
      - ./src/gainy-fetch/meltano/orchestrate:/project/orchestrate:delegated
      - ./src/gainy-fetch/meltano/seed:/project/seed:delegated
      - ./src/gainy-fetch/meltano/extract:/project/extract:delegated
      - meltano_data:/project/.meltano
    environment:
      EODHISTORICALDATA_JOBS_COUNT: 1
      AIRFLOW_PASSWORD: admin
    depends_on:
      - postgres
      - hasura
    entrypoint: "/init.sh"
    command: "invoke airflow scheduler"
    healthcheck:
      test: [ "CMD", "curl", "http://localhost:8793" ]
      interval: 30s
      retries: 3
      start_period: 30s

  meltano-airflow-ui:
    restart: always
    env_file:
      - .env
    build:
      context: ./src/gainy-fetch
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    ports:
      - "5001:5001"
    volumes:
      - ./src/gainy-fetch/meltano/orchestrate:/project/orchestrate:delegated
      - meltano_data:/project/.meltano
    depends_on:
      - postgres
      - meltano-airflow-scheduler
    entrypoint: "/wait.sh"
    command: "invoke airflow webserver -p 5001"

  meltano-dbt-docs:
    restart: always
    env_file:
      - .env
    build:
      context: ./src/gainy-fetch
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    ports:
      - "5002:5002"
    volumes:
      - ./src/gainy-fetch/meltano/scripts:/project/scripts:delegated
      - ./src/gainy-fetch/meltano/meltano.template.yml:/project/meltano.template.yml:delegated
      - ./src/gainy-fetch/meltano/transform:/project/transform:delegated
      - meltano_data:/project/.meltano
    depends_on:
      - postgres
      - meltano-airflow-scheduler
    command: "invoke dbt docs serve --port 5002"

  postgres:
    image: postgres:12
    restart: always
    volumes:
    - db_data:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: postgrespassword
    ports:
      - "5432:5432"

  hasura:
    build:
      context: ./src/hasura
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    ports:
      - "8080:8080"
      - "9693:9693"
      - "9695:9695"
    volumes:
      - ./src/hasura/hasura:/hasura
    depends_on:
      - postgres
    restart: always
    env_file:
      - .env
    environment:
      AWS_LAMBDA_API_GATEWAY_ENDPOINT: http://lambda-router:5000
      MELTANO_AIRFLOW_SCHEDULER_HOST: meltano-airflow-scheduler
      MELTANO_AIRFLOW_SCHEDULER_PORT: 8793

  firebase:
    image: ${BASE_IMAGE_REGISTRY_ADDRESS}/gainy-firebase:${BASE_IMAGE_VERSION}
    restart: always
    volumes:
      - ./src/firebase:/var/www:delegated
    ports:
      - "8081:5000" # for auth demo hosting
    env_file:
      - .env
    environment:
      GOOGLE_APPLICATION_CREDENTIALS: "/var/www/firebase.key.json"

  lambda-router:
    build:
      context: ./src/aws/router
    volumes:
      - ./src/aws/router:/app
    ports:
      - "8082:5000"

  lambda-python-trigger:
    build:
      context: ./src/aws/lambda-python
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    restart: always
    volumes:
      - ./src/aws/lambda-python/lambda_python:/var/task
      - ./src/aws/lambda-python/poetry.lock:/var/poetry.lock
      - ./src/aws/lambda-python/pyproject.toml:/var/pyproject.toml
    env_file:
      - .env
    environment:
      pg_host: "${PG_ADDRESS}"
      pg_port: "${PG_PORT}"
      pg_dbname: "${PG_DATABASE}"
      pg_username: "${PG_USERNAME}"
      pg_password: "${PG_PASSWORD}"
      AWS_LAMBDA_API_GATEWAY_PROXY_INTEGRATION: "false"
      ENV: "local"
    command: [ "hasura_handler.handle_trigger" ]

  lambda-python-action:
    build:
      context: ./src/aws/lambda-python
      args:
        BASE_IMAGE_REGISTRY_ADDRESS: ${BASE_IMAGE_REGISTRY_ADDRESS}
        BASE_IMAGE_VERSION: ${BASE_IMAGE_VERSION}
    restart: always
    volumes:
      - ./src/aws/lambda-python/lambda_python:/var/task
      - ./src/aws/lambda-python/poetry.lock:/var/poetry.lock
      - ./src/aws/lambda-python/pyproject.toml:/var/pyproject.toml
    env_file:
      - .env
    environment:
      pg_host: "${PG_ADDRESS}"
      pg_port: "${PG_PORT}"
      pg_dbname: "${PG_DATABASE}"
      pg_username: "${PG_USERNAME}"
      pg_password: "${PG_PASSWORD}"
      AWS_LAMBDA_API_GATEWAY_PROXY_INTEGRATION: "false"
      ENV: "local"
    command: [ "hasura_handler.handle_action" ]

volumes:
  db_data:
  meltano_data:
